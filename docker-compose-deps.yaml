version: "3.9"  # optional since v1.27.0

# Settings and configurations that are common for all containers
x-minio-common: &minio-common
  image: quay.io/minio/minio:RELEASE.2021-12-29T06-49-06Z
  command: server --console-address ":9001" http://minio{1...4}/data{1...2}
  expose:
    - "9000"
    - "9001"
  environment:
    MINIO_ROOT_USER: minio
    MINIO_ROOT_PASSWORD: minio123
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
    interval: 30s
    timeout: 20s
    retries: 3

x-pd-common: &pd-common
    image: pingcap/pd-arm64:v5.0.6
    volumes:
      - ./config/pd.toml:/pd.toml:ro
      - ./data:/data
      - ./logs:/logs
    restart: on-failure

x-tikv-common: &tikv-common
    image: pingcap/tikv-arm64:v5.0.6
    volumes:
      - ./config/tikv.toml:/tikv.toml:ro
      - ./data:/data
      - ./logs:/logs
    depends_on:
      - "pd0"
      - "pd1"
      - "pd2"
    restart: on-failure


services:
  pd0:
    <<: *pd-common
    command:
      - --name=pd0
      - --client-urls=http://0.0.0.0:2379
      - --peer-urls=http://0.0.0.0:2380
      - --advertise-client-urls=http://pd0:2379
      - --advertise-peer-urls=http://pd0:2380
      - --initial-cluster=pd0=http://pd0:2380,pd1=http://pd1:2380,pd2=http://pd2:2380
      - --data-dir=/data/pd0
      - --config=/pd.toml
      - --log-file=/logs/pd0.log

  pd1:
    <<: *pd-common
    command:
      - --name=pd1
      - --client-urls=http://0.0.0.0:2379
      - --peer-urls=http://0.0.0.0:2380
      - --advertise-client-urls=http://pd1:2379
      - --advertise-peer-urls=http://pd1:2380
      - --initial-cluster=pd0=http://pd0:2380,pd1=http://pd1:2380,pd2=http://pd2:2380
      - --data-dir=/data/pd1
      - --config=/pd.toml
      - --log-file=/logs/pd1.log
  pd2:
    <<: *pd-common
    command:
      - --name=pd2
      - --client-urls=http://0.0.0.0:2379
      - --peer-urls=http://0.0.0.0:2380
      - --advertise-client-urls=http://pd2:2379
      - --advertise-peer-urls=http://pd2:2380
      - --initial-cluster=pd0=http://pd0:2380,pd1=http://pd1:2380,pd2=http://pd2:2380
      - --data-dir=/data/pd2
      - --config=/pd.toml
      - --log-file=/logs/pd2.log

  tikv0:
    <<: *tikv-common
    command:
      - --addr=0.0.0.0:20160
      - --advertise-addr=tikv0:20160
      - --data-dir=/data/tikv0
      - --pd=pd0:2379,pd1:2379,pd2:2379
      - --config=/tikv.toml
      - --log-file=/logs/tikv0.log

  tikv1:
    <<: *tikv-common
    command:
      - --addr=0.0.0.0:20160
      - --advertise-addr=tikv1:20160
      - --data-dir=/data/tikv1
      - --pd=pd0:2379,pd1:2379,pd2:2379
      - --config=/tikv.toml
      - --log-file=/logs/tikv1.log

  tikv2:
    <<: *tikv-common
    command:
      - --addr=0.0.0.0:20160
      - --advertise-addr=tikv2:20160
      - --data-dir=/data/tikv2
      - --pd=pd0:2379,pd1:2379,pd2:2379
      - --config=/tikv.toml
      - --log-file=/logs/tikv2.log

  prometheus:
    user: root
    image: prom/prometheus-linux-arm64:v2.32.1
    command:
      - --log.level=error
      - --storage.tsdb.path=/data/prometheus
      - --config.file=/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./config/pd.rules.yml:/etc/prometheus/pd.rules.yml:ro
      - ./config/tikv.rules.yml:/etc/prometheus/tikv.rules.yml:ro
      - ./data:/data
    restart: on-failure

  grafana:
    image: grafana/grafana:6.0.1
    user: "0"
    environment:
      GF_LOG_LEVEL: error
      GF_PATHS_PROVISIONING: /etc/grafana/provisioning
      GF_PATHS_CONFIG: /etc/grafana/grafana.ini
    volumes:
      - ./config/grafana:/etc/grafana
      - ./config/dashboards:/tmp/dashboards
      - ./data/grafana:/var/lib/grafana
    ports:
      - "3000:3000"
    restart: on-failure

  nats1:
    image: nats:2.6-alpine
    command: --name N1 --cluster_name NUCLIA --js --sd /data --cluster nats://0.0.0.0:4245 --routes nats://nats1:4245,nats://nats2:4245,nats://nats3:4245 -p 4222
    ports:
      - "4222:4222"
    volumes:
    - jetstream-n1:/data

  nats2:
    image: nats:2.6-alpine
    command: --name N2 --cluster_name NUCLIA --js --sd /data --cluster nats://0.0.0.0:4245 --routes nats://nats1:4245,nats://nats2:4245,nats://nats3:4245 -p 4222
    volumes:
    - jetstream-n2:/data

  nats3:
    image: nats:2.6-alpine
    command: --name N3 --cluster_name NUCLIA --js --sd /data --cluster nats://0.0.0.0:4245 --routes nats://nats1:4245,nats://nats2:4245,nats://nats3:4245 -p 4222
    volumes:
    - jetstream-n3:/data

  minio1:
    <<: *minio-common
    hostname: minio1
    volumes:
      - data1-1:/data1
      - data1-2:/data2

  minio2:
    <<: *minio-common
    hostname: minio2
    volumes:
      - data2-1:/data1
      - data2-2:/data2

  minio3:
    <<: *minio-common
    hostname: minio3
    volumes:
      - data3-1:/data1
      - data3-2:/data2

  minio4:
    <<: *minio-common
    hostname: minio4
    volumes:
      - data4-1:/data1
      - data4-2:/data2

  minio:
    image: nginx:1.19.2-alpine
    hostname: minio
    volumes:
      - ./config/nginx.conf:/etc/nginx/nginx.conf:ro
    ports:
      - "9000:9000"
      - "9001:9001"
    depends_on:
      - minio1
      - minio2
      - minio3
      - minio4
  redis:
    image: arm64v8/redis:6
    command: 
    - "redis-server"
    - "--save=60"
    - "1"
    - "--loglevel warning"
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data

## By default this config uses default local driver,
## For custom volumes replace with volume driver configuration.
volumes:
  data1-1:
  data1-2:
  data2-1:
  data2-2:
  data3-1:
  data3-2:
  data4-1:
  data4-2:
  jetstream-n1:
  jetstream-n2:
  jetstream-n3:
  redis-data:
